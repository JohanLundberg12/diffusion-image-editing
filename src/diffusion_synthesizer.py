from typing import List, Optional, Tuple
import torch

from diffusers import DiffusionPipeline, UNet2DModel, VQModel, DDIMScheduler

from attr_functions import AttrFunc

from transforms import get_reverse_image_transform, reverse_transform
from utils import apply_mask


class DiffusionSynthesizer(DiffusionPipeline):
    def __init__(
        self, unet: UNet2DModel, scheduler: DDIMScheduler, vae: Optional[VQModel] = None
    ):
        super().__init__()
        self.unet = unet
        self.scheduler = scheduler
        self.vae = vae

    def encode(self, latent: torch.Tensor) -> torch.Tensor:
        with torch.no_grad():
            latent = self.vae.encode(latent)

        return latent

    def decode(self, latent) -> torch.Tensor:
        with torch.no_grad():
            latent = self.vae.decode(latent)

        return latent

    def synthesize_image(
        self,
        xt: torch.Tensor,
        x_0: Optional[torch.Tensor] = None,
        zs: Optional[List] = [],
        eta: float = 0,
        model_outputs: Optional[List] = [],
        attr_func: Optional[AttrFunc] = None,
        guidance: Optional[float] = 1.0,
        mask: Optional[torch.Tensor] = None,
        apply_mask_with_attr_func: Optional[bool] = False,
    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        new_model_outputs = list()

        for step_idx, timestep in enumerate(self.scheduler.timesteps):
            xt_cat = torch.cat([xt] * 2)

            with torch.no_grad():
                # 1. predict noise model output
                model_output = self.unet(xt_cat, timestep).sample
            model_output_res, model_output_no_res = torch.chunk(model_output, 2)

            if mask is not None and model_outputs:
                model_output_res = apply_mask(
                    mask, model_outputs[step_idx], model_output_res
                )
            model_output = model_output_no_res + guidance * (
                model_output_res - model_output_no_res
            )

            if eta > 0 and zs:
                variance_noise = zs[step_idx]
            else:
                # if none, variance_noise is generated by the pipeline in the step method
                variance_noise = None

            # nudge the latent variable x_t using the attr_func
            if attr_func is not None:
                if apply_mask_with_attr_func:
                    xt = attr_func.apply(
                        input_image=xt,
                        model_output=model_output,
                        timestep=timestep,
                        step_idx=step_idx,
                        scheduler=self.scheduler,
                        mask=mask,
                        x_0=x_0,
                    )
                else:
                    xt = attr_func.apply(
                        xt,
                        model_output,
                        timestep,
                        step_idx,
                        scheduler=self.scheduler,
                        x_0=x_0,
                    )

            # 2. predict previous mean of image xt-1 and add variance depending on eta
            # eta corresponds to Î· in paper and should be between [0, 1]
            # do xt -> xt-1
            xt = self.scheduler.step(
                model_output, timestep, xt, eta=eta, variance_noise=variance_noise
            ).prev_sample

            new_model_outputs.append(model_output)

        if self.vae is not None:
            xt = self.decode(xt)

        xt = reverse_transform(xt, get_reverse_image_transform())

        return xt, new_model_outputs
